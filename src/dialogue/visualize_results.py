"""
è¯„ä¼°ç»“æœå¯è§†åŒ–è„šæœ¬
è¯»å–è¯„ä¼°ç»“æœ JSONï¼Œç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
"""

import json
import sys
from pathlib import Path
from typing import Dict, Any, List


def load_evaluation_results(json_file: str) -> Dict[str, Any]:
    """åŠ è½½è¯„ä¼°ç»“æœ"""
    with open(json_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def print_comparison_table(results: Dict[str, Any]):
    """æ‰“å°å¯¹æ¯”è¡¨æ ¼"""
    print("\n" + "="*120)
    print("æ¨¡å‹æ€§èƒ½å¯¹æ¯”è¡¨")
    print("="*120)
    
    # è¡¨å¤´
    header = f"{'æ¨¡å‹åç§°':<35} | {'æ€è€ƒæ¨¡å¼':<18} | {'æˆåŠŸç‡':<8} | {'å¹³å‡å“åº”æ—¶é—´':<12} | {'å¹³å‡é•¿åº¦':<10} | {'å¹³å‡è´¨é‡':<10}"
    print(header)
    print("-"*120)
    
    # æ•°æ®è¡Œ
    for config in results['summary']['configurations']:
        model = config['model_name']
        thinking = config['thinking_mode']
        success = f"{config['success_rate']:.1%}"
        time_s = f"{config['avg_response_time']:.2f}s"
        length = f"{config['avg_output_length']:.0f}"
        quality = f"{config['avg_quality_score']:.3f}"
        
        row = f"{model:<35} | {thinking:<18} | {success:<8} | {time_s:<12} | {length:<10} | {quality:<10}"
        print(row)
    
    print("="*120)


def print_best_configurations(results: Dict[str, Any]):
    """æ‰“å°æœ€ä½³é…ç½®"""
    configs = results['summary']['configurations']
    
    print("\n" + "="*80)
    print("ğŸ† æœ€ä½³é…ç½®æ’è¡Œæ¦œ")
    print("="*80)
    
    # æŒ‰è´¨é‡åˆ†æ•°æ’åº
    by_quality = sorted(configs, key=lambda x: x['avg_quality_score'], reverse=True)
    print("\nã€è´¨é‡æœ€ä¼˜ã€‘å‰3å:")
    for i, config in enumerate(by_quality[:3], 1):
        print(f"  {i}. {config['model_name']} ({config['thinking_mode']}) - è´¨é‡åˆ†æ•°: {config['avg_quality_score']:.3f}")
    
    # æŒ‰å“åº”æ—¶é—´æ’åº
    by_speed = sorted(configs, key=lambda x: x['avg_response_time'])
    print("\nã€é€Ÿåº¦æœ€å¿«ã€‘å‰3å:")
    for i, config in enumerate(by_speed[:3], 1):
        print(f"  {i}. {config['model_name']} ({config['thinking_mode']}) - å“åº”æ—¶é—´: {config['avg_response_time']:.2f}s")
    
    # ç»¼åˆè¯„åˆ†ï¼ˆè´¨é‡*0.7 + é€Ÿåº¦*0.3ï¼‰
    for config in configs:
        # å½’ä¸€åŒ–é€Ÿåº¦åˆ†æ•°ï¼ˆè¶Šå¿«è¶Šå¥½ï¼‰
        max_time = max(c['avg_response_time'] for c in configs)
        speed_score = 1 - (config['avg_response_time'] / max_time)
        config['ç»¼åˆåˆ†æ•°'] = config['avg_quality_score'] * 0.7 + speed_score * 0.3
    
    by_overall = sorted(configs, key=lambda x: x['ç»¼åˆåˆ†æ•°'], reverse=True)
    print("\nã€ç»¼åˆæœ€ä½³ã€‘å‰3å (è´¨é‡70% + é€Ÿåº¦30%):")
    for i, config in enumerate(by_overall[:3], 1):
        print(f"  {i}. {config['model_name']} ({config['thinking_mode']}) - ç»¼åˆåˆ†æ•°: {config['ç»¼åˆåˆ†æ•°']:.3f}")
    
    print("="*80)


def print_thinking_mode_comparison(results: Dict[str, Any]):
    """å¯¹æ¯”åŒä¸€æ¨¡å‹çš„ä¸åŒ thinking æ¨¡å¼"""
    configs = results['summary']['configurations']
    
    print("\n" + "="*80)
    print("ğŸ’­ Thinking æ¨¡å¼å¯¹æ¯”")
    print("="*80)
    
    # æŒ‰æ¨¡å‹åˆ†ç»„
    models = {}
    for config in configs:
        model_name = config['model_name']
        if model_name not in models:
            models[model_name] = {}
        models[model_name][config['thinking_mode']] = config
    
    for model_name, modes in models.items():
        print(f"\næ¨¡å‹: {model_name}")
        
        if 'thinking_enabled' in modes and 'thinking_disabled' in modes:
            enabled = modes['thinking_enabled']
            disabled = modes['thinking_disabled']
            
            # è´¨é‡å·®å¼‚
            quality_diff = enabled['avg_quality_score'] - disabled['avg_quality_score']
            quality_pct = (quality_diff / disabled['avg_quality_score']) * 100 if disabled['avg_quality_score'] > 0 else 0
            
            # é€Ÿåº¦å·®å¼‚
            time_diff = enabled['avg_response_time'] - disabled['avg_response_time']
            time_pct = (time_diff / disabled['avg_response_time']) * 100 if disabled['avg_response_time'] > 0 else 0
            
            print(f"  è´¨é‡åˆ†æ•°: {disabled['avg_quality_score']:.3f} â†’ {enabled['avg_quality_score']:.3f} "
                  f"({'+'if quality_diff >= 0 else ''}{quality_diff:.3f}, {quality_pct:+.1f}%)")
            print(f"  å“åº”æ—¶é—´: {disabled['avg_response_time']:.2f}s â†’ {enabled['avg_response_time']:.2f}s "
                  f"({'+'if time_diff >= 0 else ''}{time_diff:.2f}s, {time_pct:+.1f}%)")
            
            # åˆ¤æ–­æ˜¯å¦å€¼å¾—å¼€å¯
            if quality_diff > 0.05 and time_pct < 50:
                print(f"  âœ“ æ¨è: å¼€å¯ thinking æ¨¡å¼ï¼ˆè´¨é‡æå‡æ˜æ˜¾ï¼Œé€Ÿåº¦æŸå¤±å¯æ¥å—ï¼‰")
            elif quality_diff < -0.02:
                print(f"  âœ— ä¸æ¨è: thinking æ¨¡å¼åè€Œé™ä½è´¨é‡")
            elif time_pct > 100:
                print(f"  âš ï¸  æƒè¡¡: thinking æ¨¡å¼æå‡è´¨é‡ï¼Œä½†é€Ÿåº¦æ…¢ä¸€å€ä»¥ä¸Š")
            else:
                print(f"  â„¹ï¸  ä¸­æ€§: thinking æ¨¡å¼å½±å“ä¸æ˜æ˜¾")
    
    print("="*80)


def print_difficulty_analysis(results: Dict[str, Any]):
    """æŒ‰éš¾åº¦åˆ†ææ€§èƒ½"""
    print("\n" + "="*80)
    print("ğŸ“Š ä¸åŒéš¾åº¦é—®é¢˜çš„æ€§èƒ½åˆ†æ")
    print("="*80)
    
    for result in results['detailed_results']:
        model_name = result['model_name']
        thinking_mode = result['thinking_mode']
        
        print(f"\n{model_name} ({thinking_mode})")
        
        if 'by_difficulty' in result['statistics']:
            by_diff = result['statistics']['by_difficulty']
            
            for difficulty in ['basic', 'intermediate', 'advanced', 'comprehensive']:
                if difficulty in by_diff:
                    data = by_diff[difficulty]
                    print(f"  {difficulty:>15}: å¹³å‡æ—¶é—´ {data['avg_response_time']:.2f}s | "
                          f"å¹³å‡è´¨é‡ {data['avg_quality_score']:.3f}")
    
    print("="*80)


def generate_markdown_report(results: Dict[str, Any], output_file: str = "evaluation_report.md"):
    """ç”Ÿæˆ Markdown æ ¼å¼çš„æŠ¥å‘Š"""
    md_lines = []
    
    md_lines.append("# æ¸…ååŠ©æ‰‹ Agent æ¨¡å‹è¯„ä¼°æŠ¥å‘Š\n")
    md_lines.append(f"**è¯„ä¼°æ—¶é—´**: {results['metadata']['evaluation_date']}\n")
    md_lines.append(f"**è¯„ä¼°æ¨¡å‹æ•°**: {results['metadata']['total_models']}\n")
    md_lines.append(f"**è¯„ä¼°é…ç½®æ•°**: {results['metadata']['total_configurations']}\n")
    md_lines.append(f"**é—®é¢˜æ•°é‡**: {results['metadata']['sample_size']}\n")
    
    # å¯¹æ¯”è¡¨æ ¼
    md_lines.append("\n## ğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”\n")
    md_lines.append("| æ¨¡å‹åç§° | æ€è€ƒæ¨¡å¼ | æˆåŠŸç‡ | å¹³å‡å“åº”æ—¶é—´ | å¹³å‡é•¿åº¦ | å¹³å‡è´¨é‡ |\n")
    md_lines.append("|---------|---------|--------|------------|---------|--------|\n")
    
    for config in results['summary']['configurations']:
        md_lines.append(
            f"| {config['model_name']} | {config['thinking_mode']} | "
            f"{config['success_rate']:.1%} | {config['avg_response_time']:.2f}s | "
            f"{config['avg_output_length']:.0f} | {config['avg_quality_score']:.3f} |\n"
        )
    
    # æœ€ä½³é…ç½®
    md_lines.append("\n## ğŸ† æœ€ä½³é…ç½®\n")
    
    configs = results['summary']['configurations']
    by_quality = sorted(configs, key=lambda x: x['avg_quality_score'], reverse=True)
    
    md_lines.append("\n### è´¨é‡æœ€ä¼˜\n")
    for i, config in enumerate(by_quality[:3], 1):
        md_lines.append(f"{i}. **{config['model_name']}** ({config['thinking_mode']}) - è´¨é‡åˆ†æ•°: {config['avg_quality_score']:.3f}\n")
    
    # ä¿å­˜æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.writelines(md_lines)
    
    print(f"\nâœ“ Markdown æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å¯è§†åŒ–è¯„ä¼°ç»“æœ')
    parser.add_argument('json_file', type=str, 
                       help='è¯„ä¼°ç»“æœ JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--markdown', action='store_true',
                       help='ç”Ÿæˆ Markdown æŠ¥å‘Š')
    
    args = parser.parse_args()
    
    # åŠ è½½ç»“æœ
    results = load_evaluation_results(args.json_file)
    
    # æ‰“å°å„ç§åˆ†æ
    print_comparison_table(results)
    print_best_configurations(results)
    print_thinking_mode_comparison(results)
    print_difficulty_analysis(results)
    
    # ç”Ÿæˆ Markdown æŠ¥å‘Š
    if args.markdown:
        generate_markdown_report(results)


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹è¾“å‡ºå¯¹æ¯”å·¥å…·
ç”¨äºæå–å’Œå¯¹æ¯”è¯„ä¼°ç»“æœä¸­ä¸åŒæ¨¡å‹å¯¹åŒä¸€é¢˜ç›®çš„å›ç­”
"""

import json
import argparse
from pathlib import Path
from typing import Dict, List, Optional


def load_evaluation_results(file_path: str) -> Dict:
    """åŠ è½½è¯„ä¼°ç»“æœJSONæ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def extract_question_outputs(results: Dict, question_id: int) -> List[Dict]:
    """
    æå–æŒ‡å®šé¢˜ç›®çš„æ‰€æœ‰æ¨¡å‹è¾“å‡º
    
    Args:
        results: è¯„ä¼°ç»“æœæ•°æ®
        question_id: é¢˜ç›®ID
        
    Returns:
        åŒ…å«æ‰€æœ‰æ¨¡å‹è¾“å‡ºçš„åˆ—è¡¨
    """
    outputs = []
    
    # éå†æ‰€æœ‰é…ç½®çš„è¯¦ç»†ç»“æœ
    for config in results.get('detailed_results', []):
        model_name = config.get('model_name', 'Unknown')
        thinking_mode = config.get('thinking_mode', 'Unknown')
        thinking_enabled = config.get('thinking_enabled', False)
        
        # æ³¨æ„ï¼šè¿™é‡Œæœ‰åµŒå¥—çš„detailed_results
        questions = config.get('detailed_results', [])
        
        # æŸ¥æ‰¾æŒ‡å®šé¢˜ç›®
        for question in questions:
            if question.get('question_id') == question_id:
                # æå–generated_answer
                output_text = question.get('output_text', '')
                generated_answer = ''
                reasoning_content = ''
                
                try:
                    # å°è¯•è§£æoutput_textä¸­çš„JSON
                    if output_text.startswith('{'):
                        output_json = json.loads(output_text)
                        data = output_json.get('data', {})
                        generated_answer = data.get('generated_answer', '')
                        reasoning_content = data.get('reasoning_content', '')
                except json.JSONDecodeError:
                    generated_answer = output_text
                
                outputs.append({
                    'model_name': model_name,
                    'thinking_mode': thinking_mode,
                    'thinking_enabled': thinking_enabled,
                    'question': question.get('question', ''),
                    'category': question.get('category', ''),
                    'difficulty': question.get('difficulty', ''),
                    'generated_answer': generated_answer,
                    'reasoning_content': reasoning_content,
                    'response_time': question.get('response_time_seconds', 0),
                    'output_length': question.get('output_length_chars', 0),
                    'quality_scores': question.get('quality_scores', {}),
                    'reference_answer': question.get('reference_answer', '')
                })
                break
    
    return outputs


def format_output_comparison(outputs: List[Dict], show_reasoning: bool = False) -> str:
    """
    æ ¼å¼åŒ–è¾“å‡ºå¯¹æ¯”ç»“æœ
    
    Args:
        outputs: æ¨¡å‹è¾“å‡ºåˆ—è¡¨
        show_reasoning: æ˜¯å¦æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
        
    Returns:
        æ ¼å¼åŒ–çš„å¯¹æ¯”æ–‡æœ¬
    """
    if not outputs:
        return "æœªæ‰¾åˆ°æŒ‡å®šé¢˜ç›®çš„è¾“å‡º"
    
    # é¢˜ç›®ä¿¡æ¯ï¼ˆæ‰€æœ‰æ¨¡å‹ç›¸åŒï¼‰
    first_output = outputs[0]
    result = []
    result.append("=" * 100)
    result.append(f"ğŸ“ é¢˜ç›® ID: {first_output.get('question', 'Unknown')}")
    result.append(f"ğŸ“š ç±»åˆ«: {first_output.get('category', '')} ({first_output.get('difficulty', '')})")
    result.append(f"âœ… å‚è€ƒç­”æ¡ˆ: {first_output.get('reference_answer', '')}")
    result.append("=" * 100)
    result.append("")
    
    # æŒ‰æ¨¡å‹æ’åº
    outputs_sorted = sorted(outputs, key=lambda x: (x['model_name'], not x['thinking_enabled']))
    
    # è¾“å‡ºæ¯ä¸ªæ¨¡å‹çš„å›ç­”
    for idx, output in enumerate(outputs_sorted, 1):
        thinking_icon = "ğŸ§ " if output['thinking_enabled'] else "âš¡"
        result.append(f"\n{'â”€' * 100}")
        result.append(f"{thinking_icon} æ¨¡å‹ {idx}: {output['model_name']} ({'æ€è€ƒæ¨¡å¼' if output['thinking_enabled'] else 'å¿«é€Ÿæ¨¡å¼'})")
        result.append(f"{'â”€' * 100}")
        result.append(f"â±ï¸  å“åº”æ—¶é—´: {output['response_time']:.2f}ç§’")
        result.append(f"ğŸ“ è¾“å‡ºé•¿åº¦: {output['output_length']}å­—")
        
        # è´¨é‡è¯„åˆ†
        quality = output['quality_scores']
        result.append(f"â­ è´¨é‡è¯„åˆ†: {quality.get('overall_quality', 0):.3f}")
        result.append(f"   â”œâ”€ é•¿åº¦é€‚å½“æ€§: {quality.get('length_appropriateness', 0):.2f}")
        result.append(f"   â”œâ”€ å…³é”®è¯è¦†ç›–: {quality.get('keyword_coverage', 0):.2f}")
        result.append(f"   â””â”€ ç»“æ„å®Œæ•´æ€§: {quality.get('structure_completeness', 0):.2f}")
        result.append("")
        result.append("ğŸ’¬ ç”Ÿæˆå›ç­”:")
        result.append("â”€" * 100)
        result.append(output['generated_answer'])
        result.append("")
        
        # å¦‚æœéœ€è¦æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
        if show_reasoning and output['reasoning_content']:
            result.append("ğŸ¤” æ¨ç†è¿‡ç¨‹:")
            result.append("â”€" * 100)
            result.append(output['reasoning_content'][:500] + "..." if len(output['reasoning_content']) > 500 else output['reasoning_content'])
            result.append("")
    
    result.append("\n" + "=" * 100)
    return "\n".join(result)


def save_comparison_to_file(comparison_text: str, output_file: str):
    """ä¿å­˜å¯¹æ¯”ç»“æœåˆ°æ–‡ä»¶"""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(comparison_text)
    print(f"\nâœ… å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def list_all_questions(results: Dict):
    """åˆ—å‡ºæ‰€æœ‰é¢˜ç›®"""
    if not results.get('detailed_results'):
        print("æœªæ‰¾åˆ°è¯„ä¼°ç»“æœ")
        return
    
    # ä»ç¬¬ä¸€ä¸ªé…ç½®ä¸­æå–æ‰€æœ‰é¢˜ç›®ï¼ˆæ³¨æ„åµŒå¥—ç»“æ„ï¼‰
    first_config = results['detailed_results'][0]
    questions = first_config.get('detailed_results', [])
    
    print("\n" + "=" * 100)
    print("ğŸ“‹ æ‰€æœ‰é¢˜ç›®åˆ—è¡¨")
    print("=" * 100)
    
    current_category = None
    for q in questions:
        category = q.get('category', '')
        if category != current_category:
            current_category = category
            print(f"\nã€{category}ã€‘")
        
        print(f"  ID {q['question_id']:2d}: {q['question']}")
        ref_answer = q['reference_answer'][:50] if q.get('reference_answer') else ''
        print(f"        éš¾åº¦: {q['difficulty']}, å‚è€ƒç­”æ¡ˆ: {ref_answer}...")
    
    print("\n" + "=" * 100)


def main():
    parser = argparse.ArgumentParser(
        description='æ¨¡å‹è¾“å‡ºå¯¹æ¯”å·¥å…· - æå–å’Œå¯¹æ¯”è¯„ä¼°ç»“æœä¸­ä¸åŒæ¨¡å‹çš„å›ç­”',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # æŸ¥çœ‹æ‰€æœ‰é¢˜ç›®
  python compare_model_outputs.py evaluation_results.json --list
  
  # å¯¹æ¯”ç¬¬1é¢˜çš„æ‰€æœ‰æ¨¡å‹è¾“å‡º
  python compare_model_outputs.py evaluation_results.json -q 1
  
  # å¯¹æ¯”ç¬¬21é¢˜å¹¶æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
  python compare_model_outputs.py evaluation_results.json -q 21 --reasoning
  
  # å¯¹æ¯”ç¬¬31é¢˜å¹¶ä¿å­˜åˆ°æ–‡ä»¶
  python compare_model_outputs.py evaluation_results.json -q 31 -o comparison_q31.txt
        """
    )
    
    parser.add_argument('results_file', 
                        help='è¯„ä¼°ç»“æœJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-q', '--question-id', 
                        type=int,
                        help='è¦å¯¹æ¯”çš„é¢˜ç›®ID')
    parser.add_argument('-o', '--output', 
                        help='ä¿å­˜å¯¹æ¯”ç»“æœçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-r', '--reasoning', 
                        action='store_true',
                        help='æ˜¾ç¤ºæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹')
    parser.add_argument('--list', 
                        action='store_true',
                        help='åˆ—å‡ºæ‰€æœ‰é¢˜ç›®')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.results_file).exists():
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {args.results_file}")
        return
    
    # åŠ è½½è¯„ä¼°ç»“æœ
    print(f"ğŸ“‚ æ­£åœ¨åŠ è½½è¯„ä¼°ç»“æœ: {args.results_file}")
    results = load_evaluation_results(args.results_file)
    print(f"âœ… å·²åŠ è½½ {len(results.get('detailed_results', []))} ä¸ªæ¨¡å‹é…ç½®çš„ç»“æœ")
    
    # å¦‚æœåªæ˜¯åˆ—å‡ºé¢˜ç›®
    if args.list:
        list_all_questions(results)
        return
    
    # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†é¢˜ç›®ID
    if args.question_id is None:
        print("\nâŒ é”™è¯¯: è¯·ä½¿ç”¨ -q å‚æ•°æŒ‡å®šé¢˜ç›®IDï¼Œæˆ–ä½¿ç”¨ --list æŸ¥çœ‹æ‰€æœ‰é¢˜ç›®")
        print("ç¤ºä¾‹: python compare_model_outputs.py evaluation_results.json -q 1")
        return
    
    # æå–æŒ‡å®šé¢˜ç›®çš„è¾“å‡º
    print(f"\nğŸ” æ­£åœ¨æå–é¢˜ç›® {args.question_id} çš„æ‰€æœ‰æ¨¡å‹è¾“å‡º...")
    outputs = extract_question_outputs(results, args.question_id)
    
    if not outputs:
        print(f"âŒ æœªæ‰¾åˆ°é¢˜ç›®ID {args.question_id} çš„è¾“å‡º")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(outputs)} ä¸ªæ¨¡å‹é…ç½®çš„è¾“å‡º")
    
    # æ ¼å¼åŒ–å¯¹æ¯”ç»“æœ
    comparison = format_output_comparison(outputs, show_reasoning=args.reasoning)
    
    # è¾“å‡ºåˆ°æ§åˆ¶å°
    print("\n" + comparison)
    
    # å¦‚æœæŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶ï¼Œä¿å­˜ç»“æœ
    if args.output:
        save_comparison_to_file(comparison, args.output)


if __name__ == '__main__':
    main()

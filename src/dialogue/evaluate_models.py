"""
æ¸…ååŠ©æ‰‹ Agent æ¨¡å‹è¯„ä¼°è„šæœ¬

è¯„ä¼°ä¸åŒæ¨¡å‹åœ¨æ¸…åæœ¬ç§‘å­¦ä¹ åŠ©æ‰‹åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°
è¯„ä¼°æŒ‡æ ‡ï¼šå“åº”æ—¶é—´ã€è¾“å‡ºé•¿åº¦ã€æ–‡æœ¬è´¨é‡
"""

import json
import time
import os
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

from .thu_agent import ThuAssistantAgent


class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    # å¾…è¯„ä¼°çš„æ¨¡å‹é…ç½®ï¼ˆåŸºäº request_param_examples.txtï¼‰
    MODELS_TO_EVALUATE = [
        {
            "name": "Doubao-seed-1-6",
            "model": "Doubao-seed-1-6",
            "model_version": "251015"
        },
        {
            "name": "Doubao-seed-1-6-flash",
            "model": "Doubao-seed-1-6-flash",
            "model_version": "250828"
        },
        {
            "name": "Doubao-seed-1-6-thinking",
            "model": "Doubao-seed-1-6-thinking",
            "model_version": "250715"
        },
        {
            "name": "Deepseek-v3-1",
            "model": "Deepseek-v3-1",
            "model_version": "250821"
        }
    ]
    
    # æ€è€ƒæ¨¡å¼é…ç½®
    THINKING_MODES = [
        {"enabled": True, "label": "thinking_enabled"},
        {"enabled": False, "label": "thinking_disabled"}
    ]
    
    def __init__(
        self,
        ak: str,
        sk: str,
        account_id: str,
        questions_file: str = "thu_agent_evaluation_questions.json",
        output_dir: str = "evaluation_results"
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            ak: ç«å±±å¼•æ“ Access Key
            sk: ç«å±±å¼•æ“ Secret Key
            account_id: è´¦æˆ· ID
            questions_file: è¯„ä¼°é¢˜åº“æ–‡ä»¶è·¯å¾„
            output_dir: ç»“æœè¾“å‡ºç›®å½•
        """
        self.ak = ak
        self.sk = sk
        self.account_id = account_id
        self.questions_file = questions_file
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åŠ è½½è¯„ä¼°é¢˜åº“
        self.questions = self._load_questions()
    
    def _load_questions(self) -> List[Dict[str, Any]]:
        """åŠ è½½è¯„ä¼°é¢˜åº“"""
        with open(self.questions_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data['questions']
    
    def _calculate_text_quality_score(
        self, 
        output: str, 
        reference: str,
        question_difficulty: str
    ) -> Dict[str, Any]:
        """
        è®¡ç®—æ–‡æœ¬è´¨é‡åˆ†æ•°
        
        æŒ‡æ ‡ï¼š
        1. é•¿åº¦åˆç†æ€§ï¼šæ ¹æ®éš¾åº¦è¯„ä¼°é•¿åº¦æ˜¯å¦åˆç†
        2. å…³é”®è¯è¦†ç›–ï¼šå‚è€ƒç­”æ¡ˆä¸­çš„å…³é”®å®ä½“æ˜¯å¦å‡ºç°
        3. ç»“æ„å®Œæ•´æ€§ï¼šæ˜¯å¦æœ‰æ¸…æ™°çš„å»ºè®®æ­¥éª¤
        """
        scores = {}
        
        # 1. é•¿åº¦åˆç†æ€§è¯„åˆ†
        output_len = len(output)
        expected_lengths = {
            "basic": (50, 200),
            "intermediate": (100, 300),
            "advanced": (200, 500),
            "comprehensive": (300, 800)
        }
        min_len, max_len = expected_lengths.get(question_difficulty, (100, 400))
        
        if output_len < min_len:
            length_score = output_len / min_len
        elif output_len > max_len:
            length_score = max(0.5, 1 - (output_len - max_len) / max_len)
        else:
            length_score = 1.0
        
        scores['length_appropriateness'] = round(length_score, 3)
        
        # 2. å…³é”®è¯è¦†ç›–ç‡
        # æå–å‚è€ƒç­”æ¡ˆä¸­çš„å…³é”®è¯ï¼ˆå»é™¤æ ‡ç‚¹ã€æ•°å­—ï¼‰
        import re
        ref_words = set(re.findall(r'[\u4e00-\u9fa5]{2,}', reference))
        out_words = set(re.findall(r'[\u4e00-\u9fa5]{2,}', output))
        
        if ref_words:
            keyword_coverage = len(ref_words & out_words) / len(ref_words)
        else:
            keyword_coverage = 0.0
        
        scores['keyword_coverage'] = round(keyword_coverage, 3)
        
        # 3. ç»“æ„å®Œæ•´æ€§ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰å»ºè®®ã€æ­¥éª¤ç­‰ï¼‰
        structure_indicators = ['å»ºè®®', 'ç¬¬ä¸€', 'ç¬¬äºŒ', 'â‘ ', 'â‘¡', 'æ­¥éª¤', 'é¦–å…ˆ', 'å…¶æ¬¡']
        structure_score = sum(1 for ind in structure_indicators if ind in output) / len(structure_indicators)
        scores['structure_completeness'] = round(structure_score, 3)
        
        # ç»¼åˆå¾—åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
        overall_score = (
            length_score * 0.3 +
            keyword_coverage * 0.5 +
            structure_score * 0.2
        )
        scores['overall_quality'] = round(overall_score, 3)
        
        return scores
    
    def _evaluate_single_question(
        self,
        model_config: Dict[str, str],
        question: Dict[str, Any],
        thinking_enabled: bool
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªé—®é¢˜
        
        Returns:
            åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        """
        question_text = question['question']
        reference_answer = question['reference_answer']
        difficulty = question['difficulty']
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        try:
            # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„ Agent å®ä¾‹ï¼ˆé¿å…å¹¶å‘ç«æ€ï¼‰
            agent = ThuAssistantAgent(
                ak=self.ak,
                sk=self.sk,
                account_id=self.account_id,
                model=model_config['model'],
                model_version=model_config['model_version']
            )
            
            # è°ƒç”¨ Agentï¼Œä¼ é€’ thinking å‚æ•°
            response = agent.query(
                user_query=question_text,
                post_process=False,  # è¯„ä¼°æ—¶ä¿ç•™åŸå§‹è¾“å‡º
                max_tokens=32768,
                temperature=1.0,
                enable_thinking=thinking_enabled
            )
            
            # è®°å½•ç»“æŸæ—¶é—´
            end_time = time.time()
            response_time = end_time - start_time
            
            # æå–å®é™…æ–‡æœ¬å†…å®¹ï¼ˆå¤„ç† JSON å“åº”ï¼‰
            try:
                response_data = json.loads(response)
                if "choices" in response_data and len(response_data["choices"]) > 0:
                    output_text = response_data["choices"][0].get("message", {}).get("content", "")
                else:
                    output_text = response
            except json.JSONDecodeError:
                output_text = response
            
            # è®¡ç®—æŒ‡æ ‡
            output_length = len(output_text)
            quality_scores = self._calculate_text_quality_score(
                output_text, 
                reference_answer,
                difficulty
            )
            
            result = {
                "question_id": question['id'],
                "question": question_text,
                "category": question['category'],
                "difficulty": difficulty,
                "response_time_seconds": round(response_time, 3),
                "output_length_chars": output_length,
                "quality_scores": quality_scores,
                "output_text": output_text[:500] + "..." if len(output_text) > 500 else output_text,  # æˆªæ–­è¿‡é•¿è¾“å‡º
                "reference_answer": reference_answer,
                "success": True,
                "error": None
            }
            
        except Exception as e:
            result = {
                "question_id": question['id'],
                "question": question_text,
                "category": question['category'],
                "difficulty": difficulty,
                "response_time_seconds": 0,
                "output_length_chars": 0,
                "quality_scores": {
                    "length_appropriateness": 0,
                    "keyword_coverage": 0,
                    "structure_completeness": 0,
                    "overall_quality": 0
                },
                "output_text": "",
                "reference_answer": reference_answer,
                "success": False,
                "error": str(e)
            }
        
        return result
    
    def _calculate_statistics(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡"""
        successful_results = [r for r in results if r['success']]
        
        if not successful_results:
            return {
                "total_questions": len(results),
                "successful_questions": 0,
                "failed_questions": len(results),
                "success_rate": 0.0
            }
        
        response_times = [r['response_time_seconds'] for r in successful_results]
        output_lengths = [r['output_length_chars'] for r in successful_results]
        quality_scores = [r['quality_scores']['overall_quality'] for r in successful_results]
        
        stats = {
            "total_questions": len(results),
            "successful_questions": len(successful_results),
            "failed_questions": len(results) - len(successful_results),
            "success_rate": round(len(successful_results) / len(results), 3),
            
            "response_time": {
                "mean": round(sum(response_times) / len(response_times), 3),
                "min": round(min(response_times), 3),
                "max": round(max(response_times), 3),
                "total": round(sum(response_times), 3)
            },
            
            "output_length": {
                "mean": round(sum(output_lengths) / len(output_lengths), 1),
                "min": min(output_lengths),
                "max": max(output_lengths),
                "total": sum(output_lengths)
            },
            
            "quality_score": {
                "mean": round(sum(quality_scores) / len(quality_scores), 3),
                "min": round(min(quality_scores), 3),
                "max": round(max(quality_scores), 3)
            }
        }
        
        # æŒ‰éš¾åº¦ç»Ÿè®¡
        by_difficulty = {}
        for difficulty in ['basic', 'intermediate', 'advanced', 'comprehensive']:
            difficulty_results = [r for r in successful_results if r['difficulty'] == difficulty]
            if difficulty_results:
                by_difficulty[difficulty] = {
                    "count": len(difficulty_results),
                    "avg_response_time": round(sum(r['response_time_seconds'] for r in difficulty_results) / len(difficulty_results), 3),
                    "avg_quality_score": round(sum(r['quality_scores']['overall_quality'] for r in difficulty_results) / len(difficulty_results), 3)
                }
        
        stats['by_difficulty'] = by_difficulty
        
        return stats
    
    def evaluate_model_config(
        self,
        model_config: Dict[str, str],
        thinking_mode: Dict[str, Any],
        sample_size: Optional[int] = None,
        max_workers: int = 8
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªæ¨¡å‹é…ç½®ï¼ˆå¹¶è¡Œæ‰§è¡Œï¼‰
        
        Args:
            model_config: æ¨¡å‹é…ç½®
            thinking_mode: æ€è€ƒæ¨¡å¼é…ç½®
            sample_size: é‡‡æ ·å¤§å°ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨è¯„ä¼°ï¼‰
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ 8ï¼‰
        
        Returns:
            è¯„ä¼°ç»“æœ
        """
        model_name = model_config['name']
        thinking_label = thinking_mode['label']
        thinking_enabled = thinking_mode['enabled']
        
        print(f"\n{'='*80}")
        print(f"è¯„ä¼°é…ç½®: {model_name} - {thinking_label}")
        print(f"{'='*80}")
        
        # é€‰æ‹©è¯„ä¼°é—®é¢˜
        questions_to_eval = self.questions[:sample_size] if sample_size else self.questions
        
        print(f"  ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶è¡Œè¯„ä¼° {len(questions_to_eval)} ä¸ªé—®é¢˜...")
        
        # å¹¶è¡Œè¯„ä¼°æ¯ä¸ªé—®é¢˜
        results = []
        completed_count = 0
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_question = {
                executor.submit(
                    self._evaluate_single_question, 
                    model_config,
                    question, 
                    thinking_enabled
                ): (i, question) 
                for i, question in enumerate(questions_to_eval, 1)
            }
            
            # æŒ‰å®Œæˆé¡ºåºå¤„ç†ç»“æœ
            for future in as_completed(future_to_question):
                i, question = future_to_question[future]
                completed_count += 1
                
                try:
                    result = future.result()
                    results.append((i, result))  # ä¿å­˜ç´¢å¼•å’Œç»“æœ
                    
                    # æ˜¾ç¤ºè¿›åº¦å’Œç»“æœ
                    print(f"  [{completed_count}/{len(questions_to_eval)}] é—®é¢˜ {question['id']}: {question['question'][:30]}...")
                    if result['success']:
                        print(f"      âœ“ è€—æ—¶: {result['response_time_seconds']:.2f}s | "
                              f"é•¿åº¦: {result['output_length_chars']} | "
                              f"è´¨é‡: {result['quality_scores']['overall_quality']:.2f}")
                    else:
                        print(f"      âœ— å¤±è´¥: {result['error']}")
                        
                except Exception as e:
                    print(f"  [{completed_count}/{len(questions_to_eval)}] é—®é¢˜ {question['id']}: æ‰§è¡Œå¼‚å¸¸ - {str(e)}")
                    # åˆ›å»ºå¤±è´¥ç»“æœ
                    results.append((i, {
                        "question_id": question['id'],
                        "question": question['question'],
                        "category": question['category'],
                        "difficulty": question['difficulty'],
                        "response_time_seconds": 0,
                        "output_length_chars": 0,
                        "quality_scores": {
                            "length_appropriateness": 0,
                            "keyword_coverage": 0,
                            "structure_completeness": 0,
                            "overall_quality": 0
                        },
                        "output_text": "",
                        "reference_answer": question['reference_answer'],
                        "success": False,
                        "error": f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"
                    }))
        
        # æŒ‰åŸå§‹é¡ºåºæ’åºç»“æœ
        results.sort(key=lambda x: x[0])
        results = [r[1] for r in results]  # åªä¿ç•™ç»“æœï¼Œå»æ‰ç´¢å¼•
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        statistics = self._calculate_statistics(results)
        
        return {
            "model_name": model_name,
            "model": model_config['model'],
            "model_version": model_config['model_version'],
            "thinking_mode": thinking_label,
            "thinking_enabled": thinking_enabled,
            "evaluation_timestamp": datetime.now().isoformat(),
            "statistics": statistics,
            "detailed_results": results
        }
    
    def run_full_evaluation(
        self,
        sample_size: Optional[int] = None,
        output_filename: Optional[str] = None,
        max_workers: int = 8
    ):
        """
        è¿è¡Œå®Œæ•´è¯„ä¼°ï¼ˆå¹¶è¡Œæ‰§è¡Œï¼‰
        
        Args:
            sample_size: æ¯ä¸ªé…ç½®çš„é‡‡æ ·å¤§å°ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨ï¼‰
            output_filename: è¾“å‡ºæ–‡ä»¶åï¼ˆNone åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ 8ï¼‰
        """
        print(f"\n{'#'*80}")
        print(f"# æ¸…ååŠ©æ‰‹ Agent æ¨¡å‹è¯„ä¼°")
        print(f"# è¯„ä¼°æ¨¡å‹æ•°: {len(self.MODELS_TO_EVALUATE)}")
        print(f"# æ€è€ƒæ¨¡å¼æ•°: {len(self.THINKING_MODES)}")
        print(f"# æ€»é…ç½®æ•°: {len(self.MODELS_TO_EVALUATE) * len(self.THINKING_MODES)}")
        print(f"# é¢˜åº“å¤§å°: {len(self.questions)}")
        print(f"# é‡‡æ ·å¤§å°: {sample_size if sample_size else 'å…¨éƒ¨'}")
        print(f"{'#'*80}\n")
        
        all_results = []
        
        # è¯„ä¼°æ¯ä¸ªæ¨¡å‹é…ç½®ç»„åˆ
        for model_config in self.MODELS_TO_EVALUATE:
            for thinking_mode in self.THINKING_MODES:
                result = self.evaluate_model_config(
                    model_config,
                    thinking_mode,
                    sample_size,
                    max_workers
                )
                all_results.append(result)
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        summary = self._generate_summary(all_results)
        
        # ä¿å­˜ç»“æœ
        if output_filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_filename = f"evaluation_results_{timestamp}.json"
        
        output_path = self.output_dir / output_filename
        
        final_output = {
            "metadata": {
                "evaluation_date": datetime.now().isoformat(),
                "total_models": len(self.MODELS_TO_EVALUATE),
                "total_configurations": len(all_results),
                "questions_file": self.questions_file,
                "sample_size": sample_size if sample_size else len(self.questions)
            },
            "summary": summary,
            "detailed_results": all_results
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(final_output, f, ensure_ascii=False, indent=2)
        
        print(f"\n{'='*80}")
        print(f"âœ“ è¯„ä¼°å®Œæˆï¼")
        print(f"âœ“ ç»“æœå·²ä¿å­˜è‡³: {output_path}")
        print(f"{'='*80}\n")
        
        # æ˜¾ç¤ºæ±‡æ€»
        self._print_summary(summary)
        
        return output_path
    
    def _generate_summary(self, all_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        summary = {
            "configurations": []
        }
        
        for result in all_results:
            config_summary = {
                "model_name": result['model_name'],
                "thinking_mode": result['thinking_mode'],
                "success_rate": result['statistics']['success_rate'],
                "avg_response_time": result['statistics']['response_time']['mean'],
                "avg_output_length": result['statistics']['output_length']['mean'],
                "avg_quality_score": result['statistics']['quality_score']['mean']
            }
            summary['configurations'].append(config_summary)
        
        # æ’åºï¼šæŒ‰è´¨é‡åˆ†æ•°é™åº
        summary['configurations'].sort(key=lambda x: x['avg_quality_score'], reverse=True)
        
        # æœ€ä½³é…ç½®
        if summary['configurations']:
            summary['best_configuration'] = summary['configurations'][0]
        
        return summary
    
    def _print_summary(self, summary: Dict[str, Any]):
        """æ‰“å°æ±‡æ€»ä¿¡æ¯"""
        print("\n" + "="*80)
        print("è¯„ä¼°ç»“æœæ±‡æ€»")
        print("="*80)
        
        for config in summary['configurations']:
            print(f"\næ¨¡å‹: {config['model_name']} ({config['thinking_mode']})")
            print(f"  æˆåŠŸç‡: {config['success_rate']:.1%}")
            print(f"  å¹³å‡å“åº”æ—¶é—´: {config['avg_response_time']:.2f}s")
            print(f"  å¹³å‡è¾“å‡ºé•¿åº¦: {config['avg_output_length']:.0f} å­—ç¬¦")
            print(f"  å¹³å‡è´¨é‡åˆ†æ•°: {config['avg_quality_score']:.3f}")
        
        if 'best_configuration' in summary:
            best = summary['best_configuration']
            print(f"\n{'='*80}")
            print(f"ğŸ† æœ€ä½³é…ç½®: {best['model_name']} ({best['thinking_mode']})")
            print(f"   è´¨é‡åˆ†æ•°: {best['avg_quality_score']:.3f}")
            print(f"{'='*80}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ¸…ååŠ©æ‰‹ Agent æ¨¡å‹è¯„ä¼°')
    parser.add_argument('--sample', type=int, default=None, 
                       help='æ¯ä¸ªé…ç½®çš„é‡‡æ ·æ•°é‡ï¼ˆé»˜è®¤å…¨éƒ¨ï¼‰')
    parser.add_argument('--output', type=str, default=None,
                       help='è¾“å‡ºæ–‡ä»¶åï¼ˆé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰')
    parser.add_argument('--workers', type=int, default=8,
                       help='å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ 8ï¼‰')
    
    args = parser.parse_args()
    
    # è·å–ç¯å¢ƒå˜é‡
    ak = os.getenv("THU_AGENT_AK")
    sk = os.getenv("THU_AGENT_SK")
    account_id = os.getenv("THU_AGENT_ACCOUNT_ID")
    
    if not all([ak, sk, account_id]):
        print("é”™è¯¯ï¼šç¼ºå°‘ç¯å¢ƒå˜é‡")
        print("è¯·è®¾ç½®: THU_AGENT_AK, THU_AGENT_SK, THU_AGENT_ACCOUNT_ID")
        return
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ModelEvaluator(
        ak=ak,
        sk=sk,
        account_id=account_id
    )
    
    # è¿è¡Œè¯„ä¼°
    evaluator.run_full_evaluation(
        sample_size=args.sample,
        output_filename=args.output,
        max_workers=args.workers
    )


if __name__ == "__main__":
    main()

"""
ä¸‹è½½ AISHELL-1 æµ‹è¯•æ•°æ®é›†
ä½¿ç”¨ Hugging Face é•œåƒåŠ é€Ÿä¸‹è½½
"""

import os
import sys
from pathlib import Path

def download_aishell1_test():
    """ä¸‹è½½ AISHELL-1 æµ‹è¯•é›†"""
    
    # è®¾ç½®é•œåƒ
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
    
    try:
        from datasets import load_dataset
    except ImportError:
        print("æ­£åœ¨å®‰è£… datasets åº“...")
        os.system("pip install datasets soundfile librosa")
        from datasets import load_dataset
    
    print("=" * 60)
    print("ğŸ“¥ ä¸‹è½½ AISHELL-1 æ•°æ®é›†")
    print("=" * 60)
    
    # ä¸‹è½½æ•°æ®é›†ï¼ˆAISHELL-1 HFç‰ˆæœ¬åªæœ‰ train splitï¼‰
    print("æ­£åœ¨ä» Hugging Face ä¸‹è½½æ•°æ®é›†...")
    dataset = load_dataset(
        "AISHELL/AISHELL-1",
        split="train"
    )
    
    print(f"âœ… ä¸‹è½½å®Œæˆï¼æ ·æœ¬æ•°: {len(dataset)}")
    print(f"   æ•°æ®å­—æ®µ: {dataset.features}")
    
    # æŸ¥çœ‹æ ·ä¾‹
    print("\nğŸ“ æ ·ä¾‹æ•°æ®:")
    for i in range(min(3, len(dataset))):
        sample = dataset[i]
        print(f"   [{i}] ID: {sample.get('id', 'N/A')}")
        print(f"       æ–‡æœ¬: {sample.get('text', sample.get('sentence', 'N/A'))}")
        print()
    
    return dataset


def evaluate_with_hf_dataset(
    models: list = None,
    num_samples: int = 100,
    device: str = None
):
    """
    ä½¿ç”¨ Hugging Face æ•°æ®é›†è¿›è¡Œè¯„ä¼°
    """
    if models is None:
        models = ['tiny', 'base', 'small']  # é»˜è®¤åªæµ‹å°æ¨¡å‹
    
    # è®¾ç½®é•œåƒ
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
    
    from datasets import load_dataset
    from huggingface_hub import hf_hub_download
    import numpy as np
    import time
    
    # æ·»åŠ é¡¹ç›®è·¯å¾„
    sys.path.insert(0, str(Path(__file__).parent.parent.parent))
    # ä½¿ç”¨ faster-whisper ä½œä¸ºè¯„ä¼°åç«¯
    try:
        from faster_whisper import WhisperModel
    except ImportError:
        print("æ­£åœ¨å®‰è£… faster-whisper ...")
        os.system("pip install faster-whisper")
        from faster_whisper import WhisperModel
    from src.asr.evaluate_asr import calculate_cer, EvaluationResult
    from src.asr.number_converter import NumberConverter
    
    # ç®€ä½“ä¸­æ–‡å¼•å¯¼æç¤ºï¼ˆè¦æ±‚ä¿æŒä¸­æ–‡æ•°å­—æ ¼å¼ï¼‰
    SIMPLIFIED_CHINESE_PROMPT = "ä»¥ä¸‹æ˜¯æ™®é€šè¯çš„å¥å­ã€‚è¯·ä½¿ç”¨ç®€ä½“ä¸­æ–‡è¾“å‡ºï¼Œä¿æŒä¸­æ–‡æ•°å­—æ ¼å¼ï¼Œä¸è¦è½¬æ¢ä¸ºé˜¿æ‹‰ä¼¯æ•°å­—ã€‚"
    
    print("=" * 60)
    print("ğŸš€ faster-whisper åŸºå‡†æµ‹è¯• (AISHELL-1)")
    print("=" * 60)
    
    # ä¸‹è½½ transcript æ–‡ä»¶
    print("æ­£åœ¨ä¸‹è½½ transcript æ–‡ä»¶...")
    transcript_path = hf_hub_download(
        repo_id="AISHELL/AISHELL-1",
        filename="data_aishell/transcript/aishell_transcript_v0.8.txt",
        repo_type="dataset"
    )
    
    # è§£æ transcript
    print("æ­£åœ¨è§£æ transcript...")
    transcripts = {}
    with open(transcript_path, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split(maxsplit=1)
            if len(parts) == 2:
                utt_id, text = parts
                # ç§»é™¤æ–‡æœ¬ä¸­çš„ç©ºæ ¼ï¼ˆä¸­æ–‡æ–‡æœ¬ï¼‰
                text = text.replace(' ', '')
                transcripts[utt_id] = text
    
    print(f"   å·²åŠ è½½ {len(transcripts)} æ¡è½¬å½•æ–‡æœ¬")
    
    # åŠ è½½æ•°æ®é›†ï¼ˆAISHELL-1 åªæœ‰ train splitï¼Œä»ä¸­æŠ½æ ·æµ‹è¯•ï¼‰
    print("æ­£åœ¨åŠ è½½éŸ³é¢‘æ•°æ®é›†...")
    dataset = load_dataset(
        "AISHELL/AISHELL-1",
        split="train"
    )
    
    print(f"   å·²åŠ è½½ {len(dataset)} æ¡éŸ³é¢‘æ ·æœ¬")
    
    # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶
    cache_file = Path(__file__).parent / "aishell1_valid_indices_cache.json"
    
    if cache_file.exists():
        print(f"âœ… å‘ç°ç¼“å­˜æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½ç´¢å¼•...")
        import json
        with open(cache_file, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
            valid_indices = cache_data['indices']
            id_map = cache_data['id_map']  # {index: utt_id}
        print(f"   ä»ç¼“å­˜åŠ è½½äº† {len(valid_indices)} æ¡æœ‰æ•ˆæ ·æœ¬ç´¢å¼•")
    else:
        # è¿‡æ»¤å‡ºæœ‰ transcript çš„æ ·æœ¬
        print("æ­£åœ¨åŒ¹é…éŸ³é¢‘å’Œæ–‡æœ¬ï¼ˆé¦–æ¬¡è¿è¡Œï¼Œæ„å»ºç´¢å¼•ç¼“å­˜ï¼‰...")
        valid_indices = []
        id_map = {}
        for idx, item in enumerate(dataset):
            if (idx + 1) % 1000 == 0:
                print(f"   è¿›åº¦: {idx+1}/{len(dataset)}, å·²åŒ¹é…: {len(valid_indices)} æ¡")
            
            key = item['__key__']
            # ä»è·¯å¾„ä¸­æå– IDï¼ˆå¦‚ train/S0002/BAC009S0002W0122 -> BAC009S0002W0122ï¼‰
            utt_id = key.split('/')[-1]
            if utt_id in transcripts:
                valid_indices.append(idx)
                id_map[str(idx)] = utt_id  # JSON key å¿…é¡»æ˜¯å­—ç¬¦ä¸²
        
        print(f"âœ… åŒ¹é…å®Œæˆï¼åŒ¹é…åˆ° {len(valid_indices)} æ¡æœ‰æ•ˆæ ·æœ¬")
        
        # ä¿å­˜ç´¢å¼•åˆ°ç¼“å­˜
        print(f"ğŸ’¾ ä¿å­˜ç´¢å¼•åˆ°ç¼“å­˜æ–‡ä»¶: {cache_file}")
        import json
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump({
                'indices': valid_indices,
                'id_map': id_map
            }, f, ensure_ascii=False, indent=2)
        print(f"   ç¼“å­˜å·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œå°†ç›´æ¥ä½¿ç”¨ç¼“å­˜ï¼ˆç§’çº§åŠ è½½ï¼‰")
    
    print(f"ğŸ“Š æœ€ç»ˆæœ‰æ•ˆæ ·æœ¬æ•°: {len(valid_indices)}")
    
    # éšæœºæŠ½æ ·ç´¢å¼•
    np.random.seed(42)
    sampled_indices = np.random.choice(
        valid_indices, 
        min(num_samples, len(valid_indices)), 
        replace=False
    )
    
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(sampled_indices)}")
    print(f"æµ‹è¯•æ¨¡å‹: {', '.join(models)}")
    print("=" * 60)
    
    results = {}
    
    for model_name in models:
        print(f"\n{'='*60}")
        print(f"ğŸ“Š è¯„ä¼°æ¨¡å‹: {model_name}")
        print(f"{'='*60}")
        
        # åˆå§‹åŒ– faster-whisper æ¨¡å‹
        try:
            # faster-whisper æ”¯æŒçš„æ¨¡å‹å: tiny, base, small, medium, large-v1, large-v2, large-v3
            # æ³¨æ„: "large" éœ€è¦æ˜¾å¼æŒ‡å®šç‰ˆæœ¬ï¼Œè¿™é‡Œä¸åšæ˜ å°„ï¼Œç”±ç”¨æˆ·æ˜ç¡®æŒ‡å®š
            use_device = device if device else ("cuda" if os.environ.get("CUDA_VISIBLE_DEVICES", "") else "cpu")
            compute_type = "float16" if (use_device == "cuda") else "int8"
            model = WhisperModel(model_name, device=use_device, compute_type=compute_type)
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            continue
        
        result = EvaluationResult(model_name=model_name)
        total_audio_duration = 0.0
        
        # ä¿å­˜æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†ç»“æœ
        sample_results = []
        
        for i, idx in enumerate(sampled_indices):
            if (i + 1) % 20 == 0:
                print(f"   è¿›åº¦: {i+1}/{len(sampled_indices)}, å½“å‰ CER: {result.cer:.2%}" if result.total_chars_ref > 0 else f"   è¿›åº¦: {i+1}/{len(sampled_indices)}")
            
            try:
                # ç›´æ¥ä» dataset ç´¢å¼•åŠ è½½æ ·æœ¬ï¼ˆæŒ‰éœ€åŠ è½½ï¼Œä¸å¤åˆ¶ï¼‰
                sample = dataset[int(idx)]
                key = sample['__key__']
                utt_id = key.split('/')[-1]
                reference = transcripts[utt_id]
                # è·å–éŸ³é¢‘æ•°æ® - Hugging Face datasets çš„ Audio å¯¹è±¡
                wav_data = sample['wav']
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å­—å…¸æ ¼å¼ï¼ˆdatasets Audio feature è‡ªåŠ¨è§£ç åçš„æ ¼å¼ï¼‰
                if isinstance(wav_data, dict) and 'array' in wav_data:
                    audio_array = wav_data['array'].astype(np.float32)
                    sample_rate = wav_data.get('sampling_rate', 16000)
                else:
                    # AudioDecoder å¯¹è±¡ - ä½¿ç”¨ torchcodec API
                    try:
                        # è°ƒç”¨ get_all_samples() è·å– AudioSamples å¯¹è±¡
                        audio_samples = wav_data.get_all_samples()
                        # AudioSamples.data æ˜¯ torch.Tensorï¼Œshape: (channels, num_samples)
                        audio_tensor = audio_samples.data
                        sample_rate = int(audio_samples.sample_rate)
                        
                        # è½¬æ¢ä¸º numpy array
                        audio_array = audio_tensor.cpu().numpy().astype(np.float32)
                        
                        # å¦‚æœæ˜¯å¤šå£°é“ (channels, samples)ï¼Œè½¬ä¸º (samples, channels) å¹¶å–ç¬¬ä¸€å£°é“
                        if audio_array.ndim == 2:
                            audio_array = audio_array[0]  # å–ç¬¬ä¸€ä¸ªå£°é“
                        
                        if i == 0:
                            print(f"   âœ… AudioDecoder: sample_rate={sample_rate}, shape={audio_array.shape}")
                            
                    except Exception as e:
                        print(f"   âš ï¸ AudioDecoder è§£ç å¤±è´¥: {e}")
                        continue
                
                # é‡é‡‡æ ·åˆ° 16kHzï¼ˆå¦‚æœéœ€è¦ï¼‰
                if sample_rate != 16000:
                    import librosa
                    audio_array = librosa.resample(audio_array, orig_sr=sample_rate, target_sr=16000)
                    sample_rate = 16000
                
                # è®¡ç®—éŸ³é¢‘æ—¶é•¿
                audio_duration = len(audio_array) / sample_rate
                total_audio_duration += audio_duration
                
                # è¯­éŸ³è¯†åˆ«ï¼ˆfaster-whisperï¼‰
                start_time = time.time()
                segments, info = model.transcribe(
                    audio_array,
                    language="zh",
                    beam_size=5,
                    vad_filter=False,
                    without_timestamps=True,
                    initial_prompt=SIMPLIFIED_CHINESE_PROMPT
                )
                hypothesis = ''.join(segment.text for segment in segments).strip()
                
                # æ•°å­—åå¤„ç†ï¼šå°†é˜¿æ‹‰ä¼¯æ•°å­—è½¬æ¢ä¸ºä¸­æ–‡æ•°å­—
                hypothesis = NumberConverter.convert_text(hypothesis)
                
                elapsed = time.time() - start_time
                # reference å·²ç»åœ¨å¾ªç¯å¼€å§‹æ—¶ä» transcripts è·å–
                
                # è®¡ç®— CER
                cer, sub, dele, ins, ref_len = calculate_cer(reference, hypothesis)
                
                # ä¿å­˜æ ·æœ¬ç»“æœ
                sample_results.append({
                    'utt_id': utt_id,
                    'reference': reference,
                    'hypothesis': hypothesis,
                    'cer': cer,
                    'substitutions': sub,
                    'deletions': dele,
                    'insertions': ins
                })
                
                # ç´¯åŠ ç»Ÿè®¡
                result.total_samples += 1
                result.total_chars_ref += ref_len
                result.total_substitutions += sub
                result.total_deletions += dele
                result.total_insertions += ins
                result.total_time += elapsed
                result.calculate_cer()
                
            except Exception as e:
                print(f"   âš ï¸ å¤„ç†æ ·æœ¬å¤±è´¥: {e}")
                continue
        
        # è®¡ç®— RTF
        if total_audio_duration > 0:
            result.rtf = result.total_time / total_audio_duration
        
        results[model_name] = result
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶ï¼ˆeval å­æ–‡ä»¶å¤¹ï¼‰
        eval_dir = Path(__file__).parent / "eval"
        eval_dir.mkdir(exist_ok=True)
        output_file = eval_dir / f"aishell1_results_{model_name}.txt"
        avg_time = result.total_time / result.total_samples if result.total_samples > 0 else 0
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("=" * 100 + "\n")
            f.write(f"AISHELL-1 è¯„ä¼°ç»“æœ - æ¨¡å‹: {model_name} (faster-whisper)\n")
            f.write(f"æ ·æœ¬æ•°: {len(sample_results)}, CER: {result.cer:.2%}, RTF: {result.rtf:.3f}, å¹³å‡è€—æ—¶: {avg_time:.3f}ç§’\n")
            f.write(f"åå¤„ç†: é˜¿æ‹‰ä¼¯æ•°å­—è½¬ä¸­æ–‡æ•°å­—\n")
            f.write("=" * 100 + "\n\n")
            
            for idx, sample in enumerate(sample_results, 1):
                f.write(f"[{idx}] {sample['utt_id']}\n")
                f.write(f"GT:  {sample['reference']}\n")
                f.write(f"è¯†åˆ«: {sample['hypothesis']}\n")
                f.write(f"CER: {sample['cer']:.2%} (æ›¿æ¢:{sample['substitutions']}, åˆ é™¤:{sample['deletions']}, æ’å…¥:{sample['insertions']})\n")
                f.write("-" * 100 + "\n")
        
        print(f"   ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # è®¡ç®—å¹³å‡è€—æ—¶
        avg_time_per_sample = result.total_time / result.total_samples if result.total_samples > 0 else 0
        
        print(f"\nğŸ“ˆ {model_name} è¯„ä¼°ç»“æœ:")
        print(f"   CER: {result.cer:.2%}")
        print(f"   RTF: {result.rtf:.3f}")
        print(f"   å¹³å‡è€—æ—¶: {avg_time_per_sample:.3f}ç§’/æ ·æœ¬")
    
    # æ‰“å°æ±‡æ€»
    print("\n" + "=" * 100)
    print("ğŸ“Š è¯„ä¼°ç»“æœæ±‡æ€» (AISHELL-1 æµ‹è¯•é›†)")
    print("=" * 100)
    print(f"{'æ¨¡å‹':<12} {'CER':>10} {'æ›¿æ¢':>8} {'åˆ é™¤':>8} {'æ’å…¥':>8} {'RTF':>8} {'å¹³å‡è€—æ—¶(ç§’)':>14}")
    print("-" * 100)
    
    for model_name, result in results.items():
        avg_time = result.total_time / result.total_samples if result.total_samples > 0 else 0
        print(f"{model_name:<12} {result.cer:>9.2%} {result.total_substitutions:>8} "
              f"{result.total_deletions:>8} {result.total_insertions:>8} "
              f"{result.rtf:>8.3f} {avg_time:>14.3f}")
    
    print("=" * 100)
    
    return results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¸‹è½½å¹¶æµ‹è¯• AISHELL-1')
    parser.add_argument('--download_only', action='store_true',
                        help='ä»…ä¸‹è½½æ•°æ®é›†')
    parser.add_argument('--models', type=str, nargs='+',
                        default=['tiny', 'base', 'small'],
                        help='è¦æµ‹è¯•çš„æ¨¡å‹')
    parser.add_argument('--num_samples', type=int, default=100,
                        help='æµ‹è¯•æ ·æœ¬æ•°')
    parser.add_argument('--device', type=str, default='cuda',
                        help='è¿è¡Œè®¾å¤‡')
    
    args = parser.parse_args()
    
    if args.download_only:
        download_aishell1_test()
    else:
        evaluate_with_hf_dataset(
            models=args.models,
            num_samples=args.num_samples,
            device=args.device
        )
